import numpy as np
import h5py
np.random.seed(1)

# Load your data in
train_dataset = h5py.File('datasets/train_data.h5', "r")
# Your training set features
train_set_x_orig = np.array(train_dataset["train_set_x"][:])
# Your training set labels
train_set_y_orig = np.array(train_dataset["train_set_y"][:])
    
dev_dataset = h5py.File('datasets/dev_data.h5', "r")
# Your development set features
dev_set_x_orig = np.array(dev_dataset["dev_set_x"][:])
# Your development set labels
dev_set_y_orig = np.array(dev_dataset["dev_set_y"][:])

test_dataset = h5py.File('datasets/test_data.h5', "r")
# Your test set features
test_set_x_orig = np.array(test_dataset["test_set_x"][:])
# Your test set labels
test_set_y_orig = np.array(test_dataset["test_set_y"][:])
    
train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
dev_set_y_orig = dev_set_y_orig.reshape((1, dev_set_y_orig.shape[0]))
test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))


# Initialize parameters
# 3 input features -- 5 hidden layers -- 2 output classifications 
# X -- your input data, numpy array of shape (n_x, m)
# a_prev -- Hidden state, numpy array of shape (n_a, m)
# Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)
# Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)
# Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)
# ba --  Bias, numpy array of shape (n_a, 1)
# by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)
m = train_set_x_orig.shape[0]
a_prev = np.random.randn(5, # of m observations)
Waa = np.random.randn(5,5)
Wax = np.random.randn(5,3)
Wya = np.random.randn(2,5)
ba = np.random.randn(5,1)
by = np.random.randn(2,1)
parameters = {"Waa": Waa, "Wax": Wax, "Wya": Wya, "ba": ba, "by": by}

# Check parameters initialization
print("Wax = " + str(parameters["Wax"]))
print("Waq = " + str(parameters["Waa"]))
print("Wya = " + str(parameters["Wy"]))
print("ba = " + str(parameters["ba"]))
print("by = " + str(parameters["by"]))

# Forward propagation
def rnn_cell_forward(X, a_prev, parameters):
  Wax = parameters["Wax"]
  Waa = parameters["Waa"]
  Wya = parameters["Wya"]
  ba = parameters["ba"]
  by = parameters["by"]
  a_next = np.tanh(np.dot(Wax, X) + np.dot(Waa, a_prev) + ba)
  yt_pred = softmax(np.dot(Wya, a_next) + by)
  cache = (a_next, a_prev, X, parameters)
  
  return a_next, yt_pred, cache

def rnn_forward(X, a0, parameters):
  caches = []
  n_x, m, T_x = train_set_x_orig.shape
  n_y, n_a = parameters["Wya"].shape
  a = np.zeros((n_a, m))
  y_pred = np.zeros((n_y, m))
  a_next = a0
  
  for t in range(len(train_set_x_orig)):
    a_next, yt_pred, cache = rnn_cell_forward(train_set_x_orig[:,:,t], a_next, parameters)
    a[:,:,t] = a_next
    y_pred[:,:,t] = yt_pred
    caches.append(cache)
    caches = (caches, x)
    return a, y_pred, caches

# Compute loss function


# Backward propagation
def rnn_cell_backward(da_next, cache):
  # Retrieve values from cache
  (a_next, a_prev, xt, parameters) = cache
  # Retrieve values from parameters
  Wax = parameters["Wax"]
  Waa = parameters["Waa"]
  Wya = parameters["Wya"]
  ba = parameters["ba"]
  by = parameters["by"]
  dtanh = (1 - a_next ** 2) * da_next
  dxt = np.dot(Wax.T, dtanh) 
  dWax = np.dot(dtanh, xt.T)
  da_prev = np.dot(Waa.T, dtanh)
  dWaa = np.dot(dtanh, a_prev.T)
  dba = np.sum(dtanh, axis = 1,keepdims=1)
  gradients = {"dxt": dxt, "da_prev": da_prev, "dWax": dWax, "dWaa": dWaa, "dba": dba}
  
  return gradients

def rnn_backward(da, caches):
  (caches, x) = caches
  (a1, a0, x1, parameters) = caches[0]
  n_a, m = da.shape
  n_x, m = x1.shape
  dx = np.zeros((n_x, m))
  dWax = np.zeros((n_a, n_x))
  dWaa = np.zeros((n_a, n_a))
  dba = np.zeros((n_a, 1))
  da0 = np.zeros((n_a, m))
  da_prevt = np.zeros((n_a, m))
  
  for t in range(len(train_set_x_orig)):
    gradients = rnn_cell_backward(da[:,:,t] + da_prevt, caches[t])
    dxt, da_prevt, dWaxt, dWaat, dbat = gradients["dxt"], gradients["da_prev"], gradients["dWax"], gradients["dWaa"], gradients["dba"]
    dx[:, :, t] = dxt
    dWax += dWaxt
    dWaa += dWaat
    dba += dbat
  
  da0 = da_prevt
  gradients = {"dx": dx, "da0": da0, "dWax": dWax, "dWaa": dWaa,"dba": dba}
  
  return gradients
    
