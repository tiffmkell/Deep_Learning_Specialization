import numpy as np
import h5py

# Load your data in
def load_data():
    train_dataset = h5py.File('datasets/train_data.h5', "r")
    # Your training set features
    train_set_x_orig = np.array(train_dataset["train_set_x"][:])
    # Your training set labels
    train_set_y_orig = np.array(train_dataset["train_set_y"][:])
    
    dev_dataset = h5py.File('datasets/dev_data.h5', "r")
    # Your development set features
    dev_set_x_orig = np.array(dev_dataset["dev_set_x"][:])
    # Your development set labels
    dev_set_y_orig = np.array(dev_dataset["dev_set_y"][:])
    
    test_dataset = h5py.File('datasets/test_data.h5', "r")
    # Your test set features
    test_set_x_orig = np.array(test_dataset["test_set_x"][:])
    # Your test set labels
    test_set_y_orig = np.array(test_dataset["test_set_y"][:])
    
    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    dev_set_y_orig = dev_set_y_orig.reshape((1, dev_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
    
    return train_set_x_orig, train_set_y_orig, dev_set_x_orig, dev_set_y_orig, test_set_x_orig, test_set_y_orig

# Define NN structure
def layer_sizes(train_set_x_orig, train_set_y_orig):
    # Size of input layer
    n_x = train_set_x_orig.shape[0] 
    # Size of hidden layer
    n_h = 1 
    # Size of output layer
    n_y = train_set_y_orig.shape[0] 
    
    return(n_x, n_h, n_y)

# Initialize parameters
def initialize_parameters(n_x, n_h, n_y):
    W1 = np.random.randn(n_h, n_x)*0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h)*0.01
    b2 = np.zeros((n_y, 1))
    
    # Assert shape
    assert(W1.shape == (n_h, n_x))
    assert(b1.shape == (n_h, 1))
    assert(W2.shape == (n_y, n_h))
    assert(b2.shape == (n_y, 1))
    
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}
    return parameters

# Check parameters initialization
parameters = initialize_parameters(n_x, n_h, n_y)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))

# Forward propagation
def forward_propagation(X, parameters):
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    
    # Implement forward prop to calculate probabilities
    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = np.sigmoid(Z2)
    
    # Assert shape of output, A2
    assert(A2.shape == (1, X.shape[1]))
    
    # Save calculated probabilities to cache
    cache = {"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2}
    return A2, cache

# Check forward calculations
A2, cache = forward_propagation(train_set_x_orig, parameters)
print(cache['Z1'], cache['A1'], cache['Z2'], cache['A2'])

# Compute loss function
def compute_cost(A2, Y, parameters):
    # Number of examples
    m = Y.shape[1]
    
    # Call from parameters
    W1 = parameters['W1']
    W2 = parameters['W2']
    
    # Compute cross-entropy cost
    logprobs = np.multiply(np.log(A2),Y) + np.multiply((1 - Y), np.log(1 - A2))
    cost = -(1/m) * np.sum(logprobs)
    
    # Assert cost dimension
    cost = np.squeeze(cost)
    assert(isinstance(cost, float))
    
    return cost

# Check loss calculation
print("cost = " + str(compute_cost(A2, Y, parameters)))

# Backwards propagation
def backward_propagation(parameters, cache, X, Y):
    # Call number of examples
    m = X.shape[1]
    
    # Call from parameters
    W1 = parameters['W1']
    W2 = parameters['W2']
    
    # Call from cache
    A1 = cache['A1']
    A2 = cache['A2']
    
    # Implement backwards prop to calculate gradients
    dZ2 = A2 - Y
    dW2 = (1/m) * np.dot(dZ2, A1.T)
    db2 = (1/m) * np.sum(dZ2, axis = 1, keepdims = True)
    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))
    dW1 = (1/m) * np.dot(dZ1, X.T)
    db1 = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)
    
    # Save calculated gradients
    grads = {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}
    return grads

# Check gradients calculations
grads = backward_propagation(parameters, cache, X, Y)
print ("dW1 = "+ str(grads["dW1"]))
print ("db1 = "+ str(grads["db1"]))
print ("dW2 = "+ str(grads["dW2"]))
print ("db2 = "+ str(grads["db2"]))

# Update parameters using gradient descent
def update_parameters(parameters, grads, learning_rate = 1.2):
    # Call from parameters
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    
    # Call from grads
    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2']
    
    # Update rule for each parameter
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    
    # Save new calculated parameters
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2}
    return parameters

# Check updated parameters calculations
parameters = update_parameters(parameters, grads)
print("W1 = " + str(parameters["W1"]))
print("b1 = " + str(parameters["b1"]))
print("W2 = " + str(parameters["W2"]))
print("b2 = " + str(parameters["b2"]))

# Model neural network
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost = False):
    # Call layer_sizes function
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Call initialize_parameters function
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    
    # Loop forward, loss, backward, update functions through iterations
    for i in range(0, num_iterations):
        # Forward propagation
        A2, cache = forward_propagation(X, parameters)
        
        # Loss
        cost = compute_cost(A2, Y, parameters)
        
        # Backwards propagation
        grads = backward_propagation(parameters, cache, X, Y)
        
        # Update parameters
        parameters = update_parameters(parameters, grads)
        
        # Print the loss every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" % (i, cost))
    
    return parameters

# Predict results
def predict(parameters, X):
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)
    
    return predictions

# Train model
parameters = nn_model(train_set_x_orig, train_set_y_orig, n_h = 1, num_iterations = 10000, print_cost = True)

# Print accuracy
predictions = predict(parameters, test_set_x_orig)
print ('Accuracy: %d' % float((np.dot(test_set_y_orig, predictions.T) + 
                               np.dot(1 - test_set_x_orig, 1 - predictions.T)) / float(test_set_x_orig.size) * 100) + '%')

# Tune hidden layer size (if accuracy is too low)
plt.figure(figsize=(16, 32))
hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]

for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(5, 2, i + 1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(dev_set_x_orig, dev_set_y_orig, n_h, num_iterations=5000)
    predictions = predict(parameters, test_set_x_orig)
    accuracy = float((np.dot(test_set_y_orig, predictions.T) + 
                      np.dot(1 - test_set_y_orig, 1 - predictions.T)) / float(test_set_y_orig.size) * 100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))
